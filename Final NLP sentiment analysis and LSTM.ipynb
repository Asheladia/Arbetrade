{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>S&amp;P 500 sentiment analysis from newsapi and reuters website based of articles of following officials and investment bankers:</h2>\n",
    "\n",
    "<ol>\n",
    "<li><p>S&amp;P 500 - news - Headlines</p></li>\n",
    "<li><p>CNBCs Jim Cramer - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>US EXPresident Donald Trump - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>JP Morgan CEO Jamie Dimon - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>Ex-Goldman Sach CEO Lloyd Blankfein - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>Goldman Sach CEO David Solomon - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>Bank of America CEO Michael Corbat - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>BlackRock CEO Larry Fink - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>FED Chairman Jerome Powell - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>FED Officials - statements - news headlines on S&amp;P 500</p></li>\n",
    "<li><p>Lead Economists - statements - news headlines on S&amp;P 500</p></li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initial imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import newsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from newsapi.newsapi_client import NewsApiClient\n",
    "from newsapi import newsapi_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mshel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from newsapi import NewsApiClient\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Headlines Sentiment\n",
    "\n",
    "Based on the news api we  pulled the latest news articles for S&P 500 and create a DataFrame of sentiment scores. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your api key environment variable\n",
    "\n",
    "api_key = \"3e339c6e32a94b3bb83af5dd6c0bc07b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a newsapi client\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start date: 2021-01-07\n",
      "\n",
      "end date: 2021-03-18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set date range for downloading headlines\n",
    "start_date = date.today() - timedelta(weeks=10)# ahere we can input exact timing we want to download data for, in days....\n",
    "end_date = date.today() #- timedelta(days=0)\n",
    "\n",
    "print(f\"\"\"\n",
    "start date: {start_date}\n",
    "\n",
    "end date: {end_date}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the S&P500 news articles\n",
    "\n",
    "#SP500_headlines = newsapi.get_sources()\n",
    "SP500_headlines = newsapi.get_everything(q=\"S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Jim Cramer statements\n",
    "\n",
    "cramer_SP500_headlines = newsapi.get_everything(q=\" Jim Cramer AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Donald Trump statements\n",
    "potus_SP500_headlines = newsapi.get_everything(q=\" Donald Trump AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Jamie Dimon statements\n",
    "dimon_SP500_headlines = newsapi.get_everything(q=\" Jamie Dimon AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Lloyd Blankfein statements\n",
    "blankfein_SP500_headlines = newsapi.get_everything(q=\"Lloyd Blankfein AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by David Solomon statements\n",
    "solomon_SP500_headlines = newsapi.get_everything(q=\"David Solomon AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Michael Corbat statements\n",
    "corbat_SP500_headlines = newsapi.get_everything(q=\"Michael Corbat AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by lead economists statements\n",
    "economists_SP500_headlines = newsapi.get_everything(q=\"Lead Economists AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Jerome Powel statements\n",
    "powel_SP500_headlines = newsapi.get_everything(q=\"Jerome Powel AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by FED statements\n",
    "fed_SP500_headlines = newsapi.get_everything(q=\"FED AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "# Fetch data on S&P 500 by Larry Fink statements\n",
    "fink_SP500_headlines = newsapi.get_everything(q=\"Larry Fink AND S&P 500\", page_size=50,language=\"en\", sort_by=\"publishedAt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles about S&P 500: 10415\n",
      "Total number of articles about S&P 500 by Jim Cramer: 59\n",
      "Total number of articles about S&P 500 by Donald Trump: 325\n",
      "Total number of articles about S&P 500 by Jamie Dimon: 13\n",
      "Total number of articles about S&P 500 by Lloyd Blankfein: 1\n",
      "Total number of articles about S&P 500 by David Solomon: 15\n",
      "Total number of articles about S&P 500 by Michael Corbat: 1\n",
      "Total number of articles about S&P 500 by lead economists: 111\n",
      "Total number of articles about S&P 500 by Jerome Powel: 0\n",
      "Total number of articles about S&P 500 by FED officials: 1967\n",
      "Total number of articles about S&P 500 by Larry Fink: 11\n"
     ]
    }
   ],
   "source": [
    "# Print all publshed articles about S&P500\n",
    "print(f\"Total number of articles about S&P 500: {SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on Jim Cramer statements\n",
    "print(f\"Total number of articles about S&P 500 by Jim Cramer: {cramer_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on Donald Trump statements\n",
    "print(f\"Total number of articles about S&P 500 by Donald Trump: {potus_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on Jamie Dimon statements\n",
    "print(f\"Total number of articles about S&P 500 by Jamie Dimon: {dimon_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on Lloyd Blankfein statements\n",
    "print(f\"Total number of articles about S&P 500 by Lloyd Blankfein: {blankfein_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on David Solomon statements\n",
    "print(f\"Total number of articles about S&P 500 by David Solomon: {solomon_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on Michael Corbat statements\n",
    "print(f\"Total number of articles about S&P 500 by Michael Corbat: {corbat_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on lead economists statements\n",
    "print(f\"Total number of articles about S&P 500 by lead economists: {economists_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on Jerome Powel statements\n",
    "print(f\"Total number of articles about S&P 500 by Jerome Powel: {powel_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on FED statements\n",
    "print(f\"Total number of articles about S&P 500 by FED officials: {fed_SP500_headlines['totalResults']}\")\n",
    "\n",
    "# Print all publshed articles about S&P500 based on larry Fink statements\n",
    "print(f\"Total number of articles about S&P 500 by Larry Fink: {fink_SP500_headlines['totalResults']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500 Compound</th>\n",
       "      <th>SP500 Negative</th>\n",
       "      <th>SP500 Neutral</th>\n",
       "      <th>SP500 Positive</th>\n",
       "      <th>SP500 Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18 18:04:34</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.070</td>\n",
       "      <td>following chair powell’s press conference yest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-18 17:52:23</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.143</td>\n",
       "      <td>photo by jetcityimage/istock editorial via get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-18 17:50:00</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.183</td>\n",
       "      <td>do you like roller coasters? according to deut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-18 17:47:00</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.183</td>\n",
       "      <td>do you like roller coasters? according to deut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-18 17:45:00</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.159</td>\n",
       "      <td>* 10-yr u.s. yield tops 1.75% as investors dig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  SP500 Compound  SP500 Negative  SP500 Neutral  \\\n",
       "0 2021-03-18 18:04:34          0.3612           0.000          0.930   \n",
       "1 2021-03-18 17:52:23          0.2732           0.088          0.769   \n",
       "2 2021-03-18 17:50:00          0.6908           0.000          0.817   \n",
       "3 2021-03-18 17:47:00          0.6908           0.000          0.817   \n",
       "4 2021-03-18 17:45:00          0.5574           0.059          0.782   \n",
       "\n",
       "   SP500 Positive                                         SP500 Text  \n",
       "0           0.070  following chair powell’s press conference yest...  \n",
       "1           0.143  photo by jetcityimage/istock editorial via get...  \n",
       "2           0.183  do you like roller coasters? according to deut...  \n",
       "3           0.183  do you like roller coasters? according to deut...  \n",
       "4           0.159  * 10-yr u.s. yield tops 1.75% as investors dig...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the S&P sentiment scores DataFrame\n",
    "SP500_sentiments = []\n",
    "\n",
    "for sp500_article in SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = sp500_article['publishedAt']\n",
    "        text = sp500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"SP500 Text\": text,\n",
    "            \"SP500 Compound\": compound,\n",
    "            \"SP500 Positive\": pos,\n",
    "            \"SP500 Negative\": neg,\n",
    "            \"SP500 Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "SP500_df = pd.DataFrame(SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\",\"SP500 Compound\",\"SP500 Negative\",\"SP500 Neutral\",\"SP500 Positive\",\"SP500 Text\"]\n",
    "SP500_df = SP500_df[cols]\n",
    "SP500_df['Date'] = pd.to_datetime(SP500_df['Date'], infer_datetime_format=True)\n",
    "#SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              datetime64[ns]\n",
       "SP500 Compound           float64\n",
       "SP500 Negative           float64\n",
       "SP500 Neutral            float64\n",
       "SP500 Positive           float64\n",
       "SP500 Text                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Cramer Compound</th>\n",
       "      <th>Cramer Negative</th>\n",
       "      <th>Cramer Neutral</th>\n",
       "      <th>Cramer Positive</th>\n",
       "      <th>Cramer Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18 05:08:00</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.095</td>\n",
       "      <td>here are five things you must know for thursda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-17 23:20:26</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.000</td>\n",
       "      <td>the federal reserve triggered an afternoon ral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-17 08:00:00</td>\n",
       "      <td>-0.2732</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.000</td>\n",
       "      <td>here's what jim cramer had to say about some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-17 05:03:00</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.037</td>\n",
       "      <td>here are five things you must know for wednesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-17 00:10:40</td>\n",
       "      <td>-0.8720</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.000</td>\n",
       "      <td>a year ago tuesday the s&amp;amp;p 500 suffered it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Cramer Compound  Cramer Negative  Cramer Neutral  \\\n",
       "0 2021-03-18 05:08:00           0.1027            0.058           0.847   \n",
       "1 2021-03-17 23:20:26          -0.4215            0.078           0.922   \n",
       "2 2021-03-17 08:00:00          -0.2732            0.054           0.946   \n",
       "3 2021-03-17 05:03:00           0.0516            0.031           0.932   \n",
       "4 2021-03-17 00:10:40          -0.8720            0.248           0.752   \n",
       "\n",
       "   Cramer Positive                                        Cramer Text  \n",
       "0            0.095  here are five things you must know for thursda...  \n",
       "1            0.000  the federal reserve triggered an afternoon ral...  \n",
       "2            0.000  here's what jim cramer had to say about some o...  \n",
       "3            0.037  here are five things you must know for wednesd...  \n",
       "4            0.000  a year ago tuesday the s&amp;p 500 suffered it...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Jim Cramer S&P sentiment scores DataFrame\n",
    "cramer_SP500_sentiments = []\n",
    "\n",
    "for cramer_sp500_article in cramer_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = cramer_sp500_article['publishedAt']\n",
    "        text = cramer_sp500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        cramer_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Cramer Text\": text,\n",
    "            \"Cramer Compound\": compound,\n",
    "            \"Cramer Positive\": pos,\n",
    "            \"Cramer Negative\": neg,\n",
    "            \"Cramer Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "cramer_SP500_df = pd.DataFrame(cramer_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Cramer Compound\",\"Cramer Negative\",\"Cramer Neutral\",\"Cramer Positive\",\"Cramer Text\"]\n",
    "cramer_SP500_df = cramer_SP500_df[cols]\n",
    "cramer_SP500_df['Date'] = pd.to_datetime(cramer_SP500_df['Date'], infer_datetime_format=True)\n",
    "#cramer_SP500_df.set_index(('Date'), inplace=True)\n",
    "cramer_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Trump Compound</th>\n",
       "      <th>Trump Negative</th>\n",
       "      <th>Trump Neutral</th>\n",
       "      <th>Trump Positive</th>\n",
       "      <th>Trump Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18 13:17:48</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.099</td>\n",
       "      <td>nasdaq and s&amp;amp;p 500 futures fell thursday a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-18 11:38:54</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.103</td>\n",
       "      <td>here are the most important news, trends and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-18 10:20:00</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.071</td>\n",
       "      <td>as the vaccine rollout ramps up, the end of co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-18 10:00:00</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.000</td>\n",
       "      <td>march 18, 6:15 a.m. colby-sawyer college has m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-18 04:00:29</td>\n",
       "      <td>0.7184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.140</td>\n",
       "      <td>today in history\\r\\ntoday is thursday, march 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Trump Compound  Trump Negative  Trump Neutral  \\\n",
       "0 2021-03-18 13:17:48         -0.0258           0.074          0.826   \n",
       "1 2021-03-18 11:38:54          0.4391           0.000          0.897   \n",
       "2 2021-03-18 10:20:00          0.4404           0.000          0.929   \n",
       "3 2021-03-18 10:00:00         -0.3612           0.079          0.921   \n",
       "4 2021-03-18 04:00:29          0.7184           0.000          0.860   \n",
       "\n",
       "   Trump Positive                                         Trump Text  \n",
       "0           0.099  nasdaq and s&amp;p 500 futures fell thursday a...  \n",
       "1           0.103  here are the most important news, trends and a...  \n",
       "2           0.071  as the vaccine rollout ramps up, the end of co...  \n",
       "3           0.000  march 18, 6:15 a.m. colby-sawyer college has m...  \n",
       "4           0.140  today in history\\r\\ntoday is thursday, march 1...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Donald Trump S&P sentiment scores DataFrame\n",
    "potus_SP500_sentiments = []\n",
    "\n",
    "for potus_sp500_article in potus_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = potus_sp500_article['publishedAt']\n",
    "        text = potus_sp500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        potus_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Trump Text\": text,\n",
    "            \"Trump Compound\": compound,\n",
    "            \"Trump Positive\": pos,\n",
    "            \"Trump Negative\": neg,\n",
    "            \"Trump Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "potus_SP500_df = pd.DataFrame(potus_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Trump Compound\",\"Trump Negative\",\"Trump Neutral\",\"Trump Positive\",\"Trump Text\"]\n",
    "potus_SP500_df = potus_SP500_df[cols]\n",
    "\n",
    "potus_SP500_df['Date'] = pd.to_datetime(potus_SP500_df['Date'], infer_datetime_format=True)\n",
    "#potus_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "potus_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dimon Compound</th>\n",
       "      <th>Dimon Negative</th>\n",
       "      <th>Dimon Neutral</th>\n",
       "      <th>Dimon Positive</th>\n",
       "      <th>Dimon Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-16 07:37:59</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.082</td>\n",
       "      <td>u.s. banks are sitting on a pile of cash that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-15 14:14:49</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>in a capital note last week, i wrote about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-15 11:44:52</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.084</td>\n",
       "      <td>u.s.banks are sitting on a pile of cash that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-15 02:16:02</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>in a capital note last week, i wrote about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-14 09:30:20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>in a capital note last week, i wrote about the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Dimon Compound  Dimon Negative  Dimon Neutral  \\\n",
       "0 2021-03-16 07:37:59          0.5106             0.0          0.918   \n",
       "1 2021-03-15 14:14:49          0.0000             0.0          1.000   \n",
       "2 2021-03-15 11:44:52          0.5106             0.0          0.916   \n",
       "3 2021-03-15 02:16:02          0.0000             0.0          1.000   \n",
       "4 2021-03-14 09:30:20          0.0000             0.0          1.000   \n",
       "\n",
       "   Dimon Positive                                         Dimon Text  \n",
       "0           0.082  u.s. banks are sitting on a pile of cash that ...  \n",
       "1           0.000  in a capital note last week, i wrote about the...  \n",
       "2           0.084  u.s.banks are sitting on a pile of cash that c...  \n",
       "3           0.000  in a capital note last week, i wrote about the...  \n",
       "4           0.000  in a capital note last week, i wrote about the...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Jamie Dimon sentiment scores DataFrame\n",
    "dimon_SP500_sentiments = []\n",
    "\n",
    "for dimon_SP500_article in dimon_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = dimon_SP500_article['publishedAt']\n",
    "        text = dimon_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        dimon_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Dimon Text\": text,\n",
    "            \"Dimon Compound\": compound,\n",
    "            \"Dimon Positive\": pos,\n",
    "            \"Dimon Negative\": neg,\n",
    "            \"Dimon Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "dimon_SP500_df = pd.DataFrame(dimon_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Dimon Compound\",\"Dimon Negative\",\"Dimon Neutral\",\"Dimon Positive\",\"Dimon Text\"]\n",
    "dimon_SP500_df = dimon_SP500_df[cols]\n",
    "\n",
    "dimon_SP500_df['Date'] = pd.to_datetime(dimon_SP500_df['Date'], infer_datetime_format=True)\n",
    "#dimon_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "dimon_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Blankfein Compound</th>\n",
       "      <th>Blankfein Negative</th>\n",
       "      <th>Blankfein Neutral</th>\n",
       "      <th>Blankfein Positive</th>\n",
       "      <th>Blankfein Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-15 04:05:06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>we are living through a stock buyback revoluti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Blankfein Compound  Blankfein Negative  \\\n",
       "0 2021-03-15 04:05:06                 0.0                 0.0   \n",
       "\n",
       "   Blankfein Neutral  Blankfein Positive  \\\n",
       "0                1.0                 0.0   \n",
       "\n",
       "                                      Blankfein Text  \n",
       "0  we are living through a stock buyback revoluti...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Lloyd Blankfein sentiment scores DataFrame\n",
    "blankfein_SP500_sentiments = []\n",
    "\n",
    "for blankfein_SP500_article in blankfein_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = blankfein_SP500_article['publishedAt']\n",
    "        text = blankfein_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        blankfein_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Blankfein Text\": text,\n",
    "            \"Blankfein Compound\": compound,\n",
    "            \"Blankfein Positive\": pos,\n",
    "            \"Blankfein Negative\": neg,\n",
    "            \"Blankfein Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "blankfein_SP500_df = pd.DataFrame(blankfein_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Blankfein Compound\",\"Blankfein Negative\",\"Blankfein Neutral\",\"Blankfein Positive\",\"Blankfein Text\"]\n",
    "blankfein_SP500_df = blankfein_SP500_df[cols]\n",
    "\n",
    "\n",
    "blankfein_SP500_df['Date'] = pd.to_datetime(blankfein_SP500_df['Date'], infer_datetime_format=True)\n",
    "#blankfein_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "blankfein_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Solomon Compound</th>\n",
       "      <th>Solomon Negative</th>\n",
       "      <th>Solomon Neutral</th>\n",
       "      <th>Solomon Positive</th>\n",
       "      <th>Solomon Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-14 22:48:10</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.119</td>\n",
       "      <td>beijing accuses the u.k. of slander over hong ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-09 09:53:00</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.181</td>\n",
       "      <td>global stocks steadied on tuesday, supported b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-09 09:25:38</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.140</td>\n",
       "      <td>by tom arnold and paulina duran\\r\\nlondon/sydn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-09 06:25:19</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.062</td>\n",
       "      <td>sydney: asian stocks recovered from earlier lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-09 04:15:31</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.058</td>\n",
       "      <td>sydney/new york: asian stocks were lower on tu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Solomon Compound  Solomon Negative  Solomon Neutral  \\\n",
       "0 2021-03-14 22:48:10            0.2732             0.063            0.818   \n",
       "1 2021-03-09 09:53:00            0.7430             0.000            0.819   \n",
       "2 2021-03-09 09:25:38            0.5994             0.000            0.860   \n",
       "3 2021-03-09 06:25:19           -0.4939             0.149            0.788   \n",
       "4 2021-03-09 04:15:31           -0.4215             0.131            0.812   \n",
       "\n",
       "   Solomon Positive                                       Solomon Text  \n",
       "0             0.119  beijing accuses the u.k. of slander over hong ...  \n",
       "1             0.181  global stocks steadied on tuesday, supported b...  \n",
       "2             0.140  by tom arnold and paulina duran\\r\\nlondon/sydn...  \n",
       "3             0.062  sydney: asian stocks recovered from earlier lo...  \n",
       "4             0.058  sydney/new york: asian stocks were lower on tu...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the David Solomon sentiment scores DataFrame\n",
    "solomon_SP500_sentiments = []\n",
    "\n",
    "for solomon_SP500_article in solomon_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = solomon_SP500_article['publishedAt']\n",
    "        text = solomon_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        solomon_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Solomon Text\": text,\n",
    "            \"Solomon Compound\": compound,\n",
    "            \"Solomon Positive\": pos,\n",
    "            \"Solomon Negative\": neg,\n",
    "            \"Solomon Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "solomon_SP500_df = pd.DataFrame(solomon_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Solomon Compound\",\"Solomon Negative\",\"Solomon Neutral\",\"Solomon Positive\",\"Solomon Text\"]\n",
    "solomon_SP500_df = solomon_SP500_df[cols]\n",
    "\n",
    "solomon_SP500_df['Date'] = pd.to_datetime(solomon_SP500_df['Date'], infer_datetime_format=True)\n",
    "#solomon_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "solomon_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Corbat Compound</th>\n",
       "      <th>Corbat Negative</th>\n",
       "      <th>Corbat Neutral</th>\n",
       "      <th>Corbat Positive</th>\n",
       "      <th>Corbat Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-28 16:26:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>citigroup\\r\\n is brokenbut the u.s. banking gi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Corbat Compound  Corbat Negative  Corbat Neutral  \\\n",
       "0 2021-02-28 16:26:00              0.0              0.0             1.0   \n",
       "\n",
       "   Corbat Positive                                        Corbat Text  \n",
       "0              0.0  citigroup\\r\\n is brokenbut the u.s. banking gi...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Michael Corbat sentiment scores DataFrame\n",
    "corbat_SP500_sentiments = []\n",
    "\n",
    "for corbat_SP500_article in corbat_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = corbat_SP500_article['publishedAt']\n",
    "        text = corbat_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        corbat_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Corbat Text\": text,\n",
    "            \"Corbat Compound\": compound,\n",
    "            \"Corbat Positive\": pos,\n",
    "            \"Corbat Negative\": neg,\n",
    "            \"Corbat Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "corbat_SP500_df = pd.DataFrame(corbat_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Corbat Compound\",\"Corbat Negative\",\"Corbat Neutral\",\"Corbat Positive\",\"Corbat Text\"]\n",
    "corbat_SP500_df = corbat_SP500_df[cols]\n",
    "\n",
    "corbat_SP500_df['Date'] = pd.to_datetime(corbat_SP500_df['Date'], infer_datetime_format=True)\n",
    "#corbat_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "corbat_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Lead economists Compound</th>\n",
       "      <th>Lead economists Negative</th>\n",
       "      <th>Lead economists Neutral</th>\n",
       "      <th>Lead economists Positive</th>\n",
       "      <th>Lead economists Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18 07:59:13</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.136</td>\n",
       "      <td>the us economy is heading for its strongest gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-18 07:08:49</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.091</td>\n",
       "      <td>summary\\r\\nsouth africa was one of the 1990s i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-18 04:21:55</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.136</td>\n",
       "      <td>the us economy is heading for its strongest gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-18 01:53:26</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.136</td>\n",
       "      <td>the us economy is heading for its strongest gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-18 01:50:22</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.088</td>\n",
       "      <td>a year after initiating zero interest rates in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Lead economists Compound  Lead economists Negative  \\\n",
       "0 2021-03-18 07:59:13                    0.6705                     0.000   \n",
       "1 2021-03-18 07:08:49                   -0.1280                     0.133   \n",
       "2 2021-03-18 04:21:55                    0.6705                     0.000   \n",
       "3 2021-03-18 01:53:26                    0.6705                     0.000   \n",
       "4 2021-03-18 01:50:22                    0.4588                     0.000   \n",
       "\n",
       "   Lead economists Neutral  Lead economists Positive  \\\n",
       "0                    0.864                     0.136   \n",
       "1                    0.776                     0.091   \n",
       "2                    0.864                     0.136   \n",
       "3                    0.864                     0.136   \n",
       "4                    0.912                     0.088   \n",
       "\n",
       "                                Lead economists Text  \n",
       "0  the us economy is heading for its strongest gr...  \n",
       "1  summary\\r\\nsouth africa was one of the 1990s i...  \n",
       "2  the us economy is heading for its strongest gr...  \n",
       "3  the us economy is heading for its strongest gr...  \n",
       "4  a year after initiating zero interest rates in...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the lead economists sentiment scores DataFrame\n",
    "economists_SP500_sentiments = []\n",
    "\n",
    "for economists_SP500_article in economists_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = economists_SP500_article['publishedAt']\n",
    "        text = economists_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        economists_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Lead economists Text\": text,\n",
    "            \"Lead economists Compound\": compound,\n",
    "            \"Lead economists Positive\": pos,\n",
    "            \"Lead economists Negative\": neg,\n",
    "            \"Lead economists Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "economists_SP500_df = pd.DataFrame(economists_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Lead economists Compound\",\"Lead economists Negative\",\"Lead economists Neutral\",\"Lead economists Positive\",\"Lead economists Text\"]\n",
    "economists_SP500_df = economists_SP500_df[cols]\n",
    "\n",
    "economists_SP500_df['Date'] = pd.to_datetime(economists_SP500_df['Date'], infer_datetime_format=True)\n",
    "#economists_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "economists_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['cols'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5846bba8e820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Powell Compound\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Powell Negative\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Powell Neutral\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Powell Positive\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Powell Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mpowel_SP500_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpowel_SP500_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cols'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mpowel_SP500_df\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpowel_SP500_df\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\algotrading\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\algotrading\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m         )\n\u001b[0;32m   1555\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\algotrading\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['cols'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Create the Jerome Powel sentiment scores DataFrame\n",
    "powel_SP500_sentiments = []\n",
    "\n",
    "for powel_SP500_article in powel_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = powel_SP500_article['publishedAt']\n",
    "        text = powel_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        powel_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Powell Text\": text,\n",
    "            \"Powell Compound\": compound,\n",
    "            \"Powell Positive\": pos,\n",
    "            \"Powell Negative\": neg,\n",
    "            \"Powell Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "powel_SP500_df = pd.DataFrame(powel_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Powell Compound\",\"Powell Negative\",\"Powell Neutral\",\"Powell Positive\",\"Powell Text\"]\n",
    "\n",
    "powel_SP500_df = powel_SP500_df[['cols']]\n",
    "\n",
    "powel_SP500_df ['Date'] = pd.to_datetime(powel_SP500_df ['Date'], infer_datetime_format=True)\n",
    "powel_SP500_df .set_index(('Date'), inplace=True)\n",
    "\n",
    "powel_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FED officials sentiment scores DataFrame\n",
    "fed_SP500_sentiments = []\n",
    "\n",
    "for fed_SP500_article in fed_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = fed_SP500_article['publishedAt']\n",
    "        text = fed_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        fed_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"FED Text\": text,\n",
    "            \"FED Compound\": compound,\n",
    "            \"FED Positive\": pos,\n",
    "            \"FED Negative\": neg,\n",
    "            \"FED Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "fed_SP500_df = pd.DataFrame(fed_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"FED Compound\",\"FED Negative\",\"FED Neutral\",\"FED Positive\",\"FED Text\"]\n",
    "fed_SP500_df = fed_SP500_df[cols]\n",
    "\n",
    "fed_SP500_df['Date'] = pd.to_datetime(fed_SP500_df['Date'], infer_datetime_format=True)\n",
    "#fed_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "fed_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Larry Fink sentiment scores DataFrame\n",
    "fink_SP500_sentiments = []\n",
    "\n",
    "for fink_SP500_article in fink_SP500_headlines[\"articles\"]:\n",
    "    try:\n",
    "        date = fink_SP500_article['publishedAt']\n",
    "        text = fink_SP500_article[\"content\"].lower()    \n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        fink_SP500_sentiments.append({\n",
    "            \"Date\": date,\n",
    "            \"Fink Text\": text,\n",
    "            \"Fink Compound\": compound,\n",
    "            \"Fink Positive\": pos,\n",
    "            \"Fink Negative\": neg,\n",
    "            \"Fink Neutral\": neu,  \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "# Create DataFrame\n",
    "fink_SP500_df = pd.DataFrame(fink_SP500_sentiments)\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "cols = [\"Date\", \"Fink Compound\",\"Fink Negative\",\"Fink Neutral\",\"Fink Positive\",\"Fink Text\"]\n",
    "fink_SP500_df = fink_SP500_df[cols]\n",
    "\n",
    "fink_SP500_df['Date'] = pd.to_datetime(fink_SP500_df['Date'], infer_datetime_format=True)\n",
    "#fink_SP500_df.set_index(('Date'), inplace=True)\n",
    "\n",
    "fink_SP500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_500_df = pd.concat([SP500_df, cramer_SP500_df, potus_SP500_df, dimon_SP500_df, blankfein_SP500_df, solomon_SP500_df, corbat_SP500_df, \n",
    "                        economists_SP500_df, powel_SP500_df,fed_SP500_df,fink_SP500_df])\n",
    "#all_500_df = all_500_df.replace(np.nan, 0)\n",
    "all_500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_500_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_500_df.to_csv(\"export_sent_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If needed\n",
    "#UPON A DECISION  POLYNOMIAL OR GET DUMMIES\n",
    "#all_500_df_dummy = pd.get_dummies(all_500_df)\n",
    "#all_500_df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_500_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the S&P 500 Sentiment by Donald Trump \n",
    "potus_SP500_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "In this section, we used NLTK and Python to tokenize the text for each option. \n",
    "1. Lowercase each word\n",
    "2. Remove Punctuation\n",
    "3. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the default stopwords list if necessary\n",
    "stop_words_ = {'cat','char', 'ha','u','wa', 'dead'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the tokenizer function\n",
    "def tokenizer(text):\n",
    "    \"\"\"Tokenizes text.\"\"\"\n",
    "    # Create a list of the words\n",
    "    sw = set(stopwords.words('english'))\n",
    "     # Remove the non-alpha characters # Substitute everything that is NOT a letter with empty string\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', text)\n",
    "    words = word_tokenize(re_clean)\n",
    "   \n",
    "    # Lemmatize Words into root words\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    #convert to lower case and remove the stopwords \n",
    "    tokens = [word.lower() for word in lem if word.lower() not in sw.union(stop_words_)]\n",
    "    \n",
    "    return tokens\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500\n",
    "SP500_tokens = SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in SP500_tokens['SP500 Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'SP500  tokens':tokenized_articles})\n",
    "\n",
    "SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "SP500_tokens['SP500  Tokens'] = SP500_tokens_df \n",
    "\n",
    "SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Jim Cramer\n",
    "\n",
    "cramer_SP500_tokens = cramer_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in cramer_SP500_tokens['Cramer Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Cramer tokens':tokenized_articles})\n",
    "\n",
    "cramer_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "cramer_SP500_tokens['Cramer Tokens'] = cramer_SP500_tokens_df \n",
    "\n",
    "cramer_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Donald Trump\n",
    "\n",
    "potus_SP500_tokens = potus_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in potus_SP500_tokens['Trump Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Trump tokens':tokenized_articles})\n",
    "\n",
    "potus_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "potus_SP500_tokens['Trump Tokens'] = potus_SP500_tokens_df \n",
    "\n",
    "potus_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Jaimie Dimon\n",
    "\n",
    "dimon_SP500_tokens = dimon_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in dimon_SP500_tokens['Dimon Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Dimon tokens':tokenized_articles})\n",
    "\n",
    "dimon_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "dimon_SP500_tokens['Dimon Tokens'] = dimon_SP500_tokens_df \n",
    "\n",
    "dimon_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Lloyd Blankfein \n",
    "\n",
    "blankfein_SP500_tokens = blankfein_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in blankfein_SP500_tokens['Blankfein Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Blankfein tokens':tokenized_articles})\n",
    "\n",
    "blankfein_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "blankfein_SP500_tokens['Blankfein Tokens'] = blankfein_SP500_tokens_df \n",
    "\n",
    "blankfein_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by David Solomon\n",
    "\n",
    "solomon_SP500_tokens = solomon_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in solomon_SP500_tokens['Solomon Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Solomon tokens':tokenized_articles})\n",
    "\n",
    "solomon_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "solomon_SP500_tokens['Solomon Tokens'] = solomon_SP500_tokens_df \n",
    "\n",
    "solomon_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Michael Corbat\n",
    "\n",
    "corbat_SP500_tokens = corbat_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in corbat_SP500_tokens['Corbat Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Corbat tokens':tokenized_articles})\n",
    "\n",
    "corbat_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "corbat_SP500_tokens['Corbat Tokens'] = corbat_SP500_tokens_df \n",
    "\n",
    "corbat_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by lead economists\n",
    "\n",
    "economists_SP500_tokens = economists_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in economists_SP500_tokens['Lead economists Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Lead economists tokens':tokenized_articles})\n",
    "\n",
    "economists_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "economists_SP500_tokens['Lead economists Tokens'] = economists_SP500_tokens_df \n",
    "\n",
    "economists_SP500_tokens.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Jerome Powell\n",
    "\n",
    "powel_SP500_tokens = powel_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in powel_SP500_tokens['Powell Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Powell tokens':tokenized_articles})\n",
    "\n",
    "powel_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "powel_SP500_tokens['Powell Tokens'] = powel_SP500_tokens_df \n",
    "\n",
    "powel_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by FED officials\n",
    "\n",
    "fed_SP500_tokens = fed_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in fed_SP500_tokens['FED Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'FED tokens':tokenized_articles})\n",
    "\n",
    "fed_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "fed_SP500_tokens['FED Tokens'] = fed_SP500_tokens_df \n",
    "\n",
    "fed_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for S&P 500 by Larry Fink\n",
    "\n",
    "fink_SP500_tokens = fink_SP500_df.copy()\n",
    "tokenized = []\n",
    "for i in fink_SP500_tokens['Fink Text']:\n",
    "    tokenized_articles = tokenizer(i)\n",
    "    tokenized.append({'Fink tokens':tokenized_articles})\n",
    "\n",
    "fink_SP500_tokens_df= pd.DataFrame(tokenized)\n",
    "fink_SP500_tokens['FED Tokens'] = fink_SP500_tokens_df \n",
    "\n",
    "fink_SP500_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_df = pd.concat([SP500_tokens, cramer_SP500_tokens, potus_SP500_tokens, dimon_SP500_tokens, blankfein_SP500_tokens, solomon_SP500_tokens, \n",
    "                           corbat_SP500_tokens, economists_SP500_tokens, powel_SP500_tokens, fed_SP500_tokens, fink_SP500_tokens])\n",
    "\n",
    "all_tokens_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the database for polynomial features if we decide to use it\n",
    "all_ = all_500_df.drop(columns =['SP500 Text', 'Cramer Text', 'Trump Text', 'Dimon Text', 'Blankfein Text', \n",
    "                                 'Solomon Text', 'Corbat Text', 'Lead economists Text', 'Powell Text', 'FED Text', 'Fink Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPON A DECISION  POLYNOMIAL OR GET DUMMIES\n",
    "#in case we wanted to use as a preporcesing polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "print(poly.fit_transform(all_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGrams and Frequency Analysis\n",
    "\n",
    "In this section we are looking at the ngrams and word frequency for each option. \n",
    "\n",
    "1. Use NLTK to produce the n-grams for N = 2. \n",
    "2. List the top 10 words for each option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2\n",
    "SP500_words_all = []\n",
    "for text in SP500_tokens['SP500  Tokens']:\n",
    "    for word in text:\n",
    "        SP500_words_all.append(word)\n",
    "SP500_word_counts = Counter(ngrams(SP500_words_all, n=2))\n",
    "SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by FED officials\n",
    "fed_SP500_words_all = []\n",
    "for text in fed_SP500_tokens['FED Tokens']:\n",
    "    for word in text:\n",
    "        fed_SP500_words_all.append(word)\n",
    "fed_SP500_word_counts = Counter(ngrams(fed_SP500_words_all, n=2))\n",
    "fed_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by Donald Trump\n",
    "potus_SP500_words_all = []\n",
    "for text in potus_SP500_tokens['Trump Tokens']:\n",
    "    for word in text:\n",
    "        potus_SP500_words_all.append(word)\n",
    "potus_SP500_word_counts = Counter(ngrams(potus_SP500_words_all, n=2))\n",
    "potus_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by Jamie Dimon\n",
    "dimon_SP500_words_all = []\n",
    "for text in dimon_SP500_tokens['Dimon Tokens']:\n",
    "    for word in text:\n",
    "        dimon_SP500_words_all.append(word)\n",
    "dimon_SP500_word_counts = Counter(ngrams(dimon_SP500_words_all, n=3))\n",
    "dimon_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by Lloyd Blankfein\n",
    "blankfein_SP500_words_all = []\n",
    "for text in blankfein_SP500_tokens['Blankfein Tokens']:\n",
    "    for word in text:\n",
    "        blankfein_SP500_words_all.append(word)\n",
    "blankfein_SP500_word_counts = Counter(ngrams(blankfein_SP500_words_all, n=3))\n",
    "blankfein_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by David Solomon\n",
    "solomon_SP500_words_all = []\n",
    "for text in solomon_SP500_tokens['Solomon Tokens']:\n",
    "    for word in text:\n",
    "        solomon_SP500_words_all.append(word)\n",
    "solomon_SP500_word_counts = Counter(ngrams(solomon_SP500_words_all, n=3))\n",
    "solomon_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by Michael Corbat\n",
    "corbat_SP500_words_all = []\n",
    "for text in corbat_SP500_tokens['Corbat Tokens']:\n",
    "    for word in text:\n",
    "        corbat_SP500_words_all.append(word)\n",
    "corbat_SP500_word_counts = Counter(ngrams(corbat_SP500_words_all, n=2))\n",
    "corbat_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 N-grams where N=2 by lead economists\n",
    "economists_SP500_words_all = []\n",
    "for text in economists_SP500_tokens['Lead economists Tokens']:\n",
    "    for word in text:\n",
    "        economists_SP500_words_all.append(word)\n",
    "economists_SP500_word_counts = Counter(ngrams(economists_SP500_words_all, n=2))\n",
    "economists_SP500_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the token_count function to generate the top 10 words from each option\n",
    "tokens = all_tokens_df\n",
    "def token_count(tokens, N=10):\n",
    "    \n",
    "    \"\"\"Returns the top N tokens from the frequency count\"\"\"\n",
    "    \n",
    "    return Counter(tokens).most_common(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 words for S&P 500\n",
    "\n",
    "token_count(SP500_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 words for S&P 500 by Donald Trump\n",
    "token_count(potus_SP500_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds\n",
    "\n",
    "In this section, we will generate word clouds for each option to summarize the news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [20.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the function\n",
    "def processed_text(corpus): \n",
    "    # Combine all articles in corpus into one large string\n",
    "    big_string = ' '.join(corpus)\n",
    "    return big_string\n",
    "SP500_words = processed_text(SP500_words_all)\n",
    "\n",
    "potus_SP500_words = processed_text(potus_SP500_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 word cloud\n",
    "SP500_wc = WordCloud(collocations=False).generate(SP500_words)\n",
    "fig = plt.figure()\n",
    "plt.imshow(SP500_wc)\n",
    "plt.title('Word Cloud - S&P 500', fontsize=30, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the S&P 500 by Donald Trump word cloud\n",
    "\n",
    "fig = plt.figure()\n",
    "potus_SP500_wc = WordCloud().generate(potus_SP500_words)\n",
    "plt.imshow(potus_SP500_wc)\n",
    "plt.title('Word Cloud - S&P 500 by The President - Mr. Donald Trump', fontsize=30, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition for all chosen options for S&P 500\n",
    "\n",
    "In this section, we built a named entity recognition model for all options and visualized the tags using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the S&P 500 text together\n",
    "SP500_sent = []\n",
    "for sent in SP500_tokens['SP500 Text']:\n",
    "    sent_list = sent\n",
    "    SP500_sent.append(sent_list)\n",
    "    \n",
    "one_string_SP500 = ' '.join(SP500_sent)\n",
    "one_string_SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "SP500_doc = nlp(one_string_SP500)\n",
    "\n",
    "# Add a title to the document\n",
    "SP500_doc.user_data['title'] = 'S&P 500 NER'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(SP500_doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Entities\n",
    "for ent in SP500_doc.ents:\n",
    "    print (ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 by Jim Cramer NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the Jim Cramer text together\n",
    "cramer_SP500_sent = []\n",
    "for sent in cramer_SP500_tokens['Cramer Text']:\n",
    "    sent_list = sent\n",
    "    cramer_SP500_sent.append(sent_list)\n",
    "    \n",
    "one_string_cramer_SP500 = ' '.join(cramer_SP500_sent)\n",
    "one_string_cramer_SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "cramer_SP500_doc = nlp(one_string_cramer_SP500)\n",
    "\n",
    "# Add a title to the document\n",
    "cramer_SP500_doc.user_data['title'] = 'S&P 500 NER by Jim Cramer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(cramer_SP500_doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Entities\n",
    "for ent in cramer_SP500_doc.ents:\n",
    "    print (ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 NER by The President Mr. Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the Donald Trump text together\n",
    "potus_SP500_sent = []\n",
    "for sent in potus_SP500_tokens['Trump Text']:\n",
    "    sent_list = sent\n",
    "    potus_SP500_sent.append(sent_list)\n",
    "    \n",
    "one_string_potus_SP500 = ' '.join(potus_SP500_sent)\n",
    "one_string_potus_SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "potus_SP500_doc = nlp(one_string_potus_SP500)\n",
    "\n",
    "# Add a title to the document\n",
    "potus_SP500_doc.user_data['title'] = 'S&P 500 NER by Donald Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(potus_SP500_doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised LSTM Model 1 (Single Neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Additional Imports\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "model_df4 = pd.read_csv('final_db_spy_sent.csv')\n",
    "model_df4.set_index((\"Date\"), inplace=True)\n",
    "model_df4.drop(columns=['Daily Returns', 'Cumulative Returns'], inplace=True)\n",
    "data4 = model_df4.fillna(model_df4.mean())\n",
    "data4.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Set target Data which is the SPY_Close and normalize features data\n",
    "dataset = data4[\"SPY Close\"].values\n",
    "dataset = np.reshape(dataset, (-1, 1))\n",
    "# Use MinMaxScaler to scale data to values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "# Scale the dataset\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# Use 80% of data for training and the remainder for testing\n",
    "train_size = int(len(dataset) * 0.80)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        features = dataset[i:(i+look_back), 0]\n",
    "        target = dataset[i + look_back, 0]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape features data to be a vertical vector to be compatible with LSTM API\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "#X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "#X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "#from keras.layers.recurrent import LSTM\n",
    "# Define The LSTM RNN Model\n",
    "model = Sequential()\n",
    "\n",
    "number_units = 100\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "#Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units, \n",
    "    return_sequences=True,\n",
    "    #Input shape is 1 time hop with features data =16\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "# Fit the Model\n",
    "history = model.fit(X_train, y_train, epochs=10, shuffle=False, batch_size=2, verbose=2 )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "predicted_prices = scaler.inverse_transform(predicted.reshape(-1,1))\n",
    "actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of Real and Predicted Values\n",
    "model_1_lstm = pd.DataFrame({\n",
    "    \"Actual\": actual_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "})\n",
    "model_1_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_lstm.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised LSTM Model Multi layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Imports\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import math\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "model_df5 = pd.read_csv('final_db_spy_sent.csv')\n",
    "model_df5.set_index((\"Date\"), inplace=True)\n",
    "model_df5.drop(columns=['Daily Returns', 'Cumulative Returns'], inplace=True)\n",
    "data5 = model_df5\n",
    "print(\"Number of rows and columns:\", data5.shape)\n",
    "data5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility - comment out until final model version\n",
    "#from numpy.random import seed\n",
    "#seed(1)\n",
    "#from tensorflow import random\n",
    "#random.set_seed(3)\n",
    "\n",
    "# Set target Data which is the SPY_Close and normalize features data\n",
    "dataset = data5[\"SPY Close\"].values\n",
    "dataset = np.reshape(dataset, (-1, 1))\n",
    "# Use MinMaxScaler to scale data to values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "# Scale the dataset\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# Use 80% of data for training and the remainder for testing\n",
    "train_size = int(len(dataset) * 0.80)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back =1):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        features = dataset[i:(i+look_back), 0]\n",
    "        target = dataset[i + look_back, 0]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape features data to be a vertical vector to be compatible with LSTM API\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "#X1_train = X_train.reshape((X1_train.shape[0], X_train.shape[1], 1))\n",
    "#X1_test = X_test.reshape((X1_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define The LSTM RNN Model\n",
    "model2 = Sequential()\n",
    "\n",
    "number_units = 100\n",
    "dropout_fraction = 0.2\n",
    "# Layer 1\n",
    "model2.add(LSTM(units=number_units,\n",
    "                return_sequences=True,\n",
    "    #Input shape is 1 time hop with features data of 16\n",
    "    input_shape = (X_train.shape[1], 1)))\n",
    "model2.add(Dropout(dropout_fraction))\n",
    "# Layer 2\n",
    "model2.add(LSTM(units=number_units,\n",
    "               return_sequences=True))\n",
    "model2.add(Dropout(dropout_fraction))\n",
    "# Layer 3\n",
    "model2.add(LSTM(units=number_units))\n",
    "model2.add(Dropout(dropout_fraction))\n",
    "# Output Layer\n",
    "model2.add(Dense(1))\n",
    "\n",
    "#Compile the Model \n",
    "model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the Model\n",
    "history = model2.fit(X_train, y_train, epochs=15, shuffle=False, batch_size=2, verbose=1 )\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model2.predict(X_test)\n",
    "\n",
    "predicted_prices = scaler.inverse_transform(predicted.reshape(-1,1))\n",
    "actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of Real and Predicted Values\n",
    "model_2_lstm = pd.DataFrame({\n",
    "    \"Actual\": actual_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "})\n",
    "model_2_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_lstm.plot(figsize=(15,8))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:algotrading]",
   "language": "python",
   "name": "conda-env-algotrading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
